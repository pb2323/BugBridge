"""
Reporting Agent

Generates daily summary reports with metrics, trends, and natural language summaries.
"""

from __future__ import annotations

from datetime import UTC, datetime, timedelta
from typing import Any, Dict, List, Literal, Optional

from pydantic import BaseModel, Field
from sqlalchemy import func, select
from sqlalchemy.ext.asyncio import AsyncSession

from bugbridge.agents.base import BaseAgent
from bugbridge.config import get_settings
from bugbridge.database.models import (
    FeedbackPost as DBFeedbackPost,
    WorkflowState as DBWorkflowState,
    JiraTicket as DBJiraTicket,
    Report,
)
from bugbridge.database.connection import get_session_context
from bugbridge.integrations.email import EmailService, EmailDeliveryError
from bugbridge.integrations.file_storage import FileStorageService, FileStorageError
from bugbridge.integrations.xai import ChatXAI
from bugbridge.models.report_filters import ReportFilters
from bugbridge.utils.logging import get_logger
from sqlalchemy.dialects.postgresql import ARRAY

logger = get_logger(__name__)


class DailyMetrics(BaseModel):
    """
    Aggregated daily metrics for reporting.

    Attributes:
        date: Report date
        new_issues_count: Number of new feedback posts collected
        bugs_count: Number of bugs detected
        feature_requests_count: Number of feature requests
        bugs_percentage: Percentage of bugs vs total issues
        sentiment_distribution: Distribution of sentiment categories
        priority_items: Top priority items requiring attention
        tickets_created: Number of Jira tickets created
        tickets_resolved: Number of Jira tickets resolved
        average_response_time_hours: Average time from feedback to ticket creation (hours)
        resolution_rate: Percentage of tickets resolved
        average_resolution_time_hours: Average time to resolution (hours)
    """

    date: datetime
    new_issues_count: int = Field(..., ge=0)
    bugs_count: int = Field(..., ge=0)
    feature_requests_count: int = Field(..., ge=0)
    bugs_percentage: float = Field(..., ge=0.0, le=100.0)
    sentiment_distribution: Dict[str, int] = Field(default_factory=dict)
    priority_items: List[Dict[str, Any]] = Field(default_factory=list)
    tickets_created: int = Field(..., ge=0)
    tickets_resolved: int = Field(..., ge=0)
    average_response_time_hours: Optional[float] = Field(None, ge=0.0)
    resolution_rate: float = Field(..., ge=0.0, le=100.0)
    average_resolution_time_hours: Optional[float] = Field(None, ge=0.0)


class ReportSummary(BaseModel):
    """
    Natural language summary generated by XAI LLM.

    Attributes:
        executive_summary: High-level overview of the day's activity
        key_insights: List of key insights and trends
        recommendations: Recommendations for action items
        summary_text: Complete natural language summary
    """

    executive_summary: str = Field(..., min_length=50)
    key_insights: List[str] = Field(..., min_items=1)
    recommendations: List[str] = Field(default_factory=list)
    summary_text: str = Field(..., min_length=100)


async def query_daily_metrics(
    session: AsyncSession,
    report_date: datetime,
    filters: Optional[ReportFilters] = None,
) -> DailyMetrics:
    """
    Query and aggregate daily metrics from database with optional filters.

    Args:
        session: Database session.
        report_date: Date for which to generate metrics (used if filters not provided).
        filters: Optional ReportFilters to customize the query.

    Returns:
        DailyMetrics object with aggregated data.
    """
    # Use filters if provided, otherwise use report_date
    if filters:
        start_date, end_date = filters.to_date_range()
        if start_date is None:
            # Default to start of report_date if no start_date in filters
            start_date = report_date.replace(hour=0, minute=0, second=0, microsecond=0)
        if end_date is None:
            # Default to end of report_date if no end_date in filters
            end_date = report_date.replace(hour=23, minute=59, second=59, microsecond=999999)
    else:
        # Calculate date range for the day
        start_date = report_date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = start_date + timedelta(days=1)

    # Build base query conditions
    base_conditions = [
        DBFeedbackPost.collected_at >= start_date,
        DBFeedbackPost.collected_at <= end_date,
    ]

    # Apply filters
    if filters:
        if filters.board_ids:
            base_conditions.append(DBFeedbackPost.board_id.in_(filters.board_ids))
        if filters.tags:
            # PostgreSQL array overlap: check if any tag in filters.tags exists in DBFeedbackPost.tags
            # Use array overlap operator: tags && ARRAY['tag1', 'tag2']
            from sqlalchemy.dialects.postgresql import array
            base_conditions.append(DBFeedbackPost.tags.op('&&')(array(filters.tags)))
        if filters.statuses:
            base_conditions.append(DBFeedbackPost.status.in_(filters.statuses))
        if filters.min_votes is not None:
            base_conditions.append(DBFeedbackPost.votes >= filters.min_votes)

    # Query new feedback posts with filters
    new_posts_query = select(func.count(DBFeedbackPost.id)).where(*base_conditions)
    new_issues_count = (await session.execute(new_posts_query)).scalar() or 0

    # Query bugs vs feature requests
    from bugbridge.database.models import AnalysisResult

    # Build bug query with filters
    bug_query_conditions = [
        AnalysisResult.feedback_post_id == DBFeedbackPost.id,
        DBFeedbackPost.collected_at >= start_date,
        DBFeedbackPost.collected_at <= end_date,
        AnalysisResult.is_bug == True,  # noqa: E712
    ]

    # Apply filters to bug query
    if filters:
        if filters.board_ids:
            bug_query_conditions.append(DBFeedbackPost.board_id.in_(filters.board_ids))
        if filters.tags:
            from sqlalchemy.dialects.postgresql import array
            bug_query_conditions.append(DBFeedbackPost.tags.op('&&')(array(filters.tags)))
        if filters.statuses:
            bug_query_conditions.append(DBFeedbackPost.status.in_(filters.statuses))
        if filters.min_votes is not None:
            bug_query_conditions.append(DBFeedbackPost.votes >= filters.min_votes)
        if filters.bug_only:
            # Already filtering by is_bug == True
            pass
        if filters.feature_only:
            # This query is for bugs, so skip if feature_only
            bugs_count = 0
        else:
            bugs_query = select(func.count(AnalysisResult.id)).join(
                DBFeedbackPost, AnalysisResult.feedback_post_id == DBFeedbackPost.id
            ).where(*bug_query_conditions)
            bugs_count = (await session.execute(bugs_query)).scalar() or 0
    else:
        bugs_query = select(func.count(AnalysisResult.id)).join(
            DBFeedbackPost, AnalysisResult.feedback_post_id == DBFeedbackPost.id
        ).where(*bug_query_conditions)
        bugs_count = (await session.execute(bugs_query)).scalar() or 0

    # Query feature requests (non-bugs)
    if filters and filters.bug_only:
        feature_requests_count = 0
    else:
        feature_query_conditions = [
            AnalysisResult.feedback_post_id == DBFeedbackPost.id,
            DBFeedbackPost.collected_at >= start_date,
            DBFeedbackPost.collected_at <= end_date,
            AnalysisResult.is_bug == False,  # noqa: E712
        ]

        if filters:
            if filters.board_ids:
                feature_query_conditions.append(DBFeedbackPost.board_id.in_(filters.board_ids))
            if filters.tags:
                from sqlalchemy.dialects.postgresql import array
                feature_query_conditions.append(DBFeedbackPost.tags.op('&&')(array(filters.tags)))
            if filters.statuses:
                feature_query_conditions.append(DBFeedbackPost.status.in_(filters.statuses))
            if filters.min_votes is not None:
                feature_query_conditions.append(DBFeedbackPost.votes >= filters.min_votes)

        feature_query = select(func.count(AnalysisResult.id)).join(
            DBFeedbackPost, AnalysisResult.feedback_post_id == DBFeedbackPost.id
        ).where(*feature_query_conditions)
        feature_requests_count = (await session.execute(feature_query)).scalar() or 0

    bugs_percentage = (bugs_count / new_issues_count * 100) if new_issues_count > 0 else 0.0

    # Query sentiment distribution
    from bugbridge.database.models import AnalysisResult

    sentiment_query_conditions = [
        AnalysisResult.feedback_post_id == DBFeedbackPost.id,
        DBFeedbackPost.collected_at >= start_date,
        DBFeedbackPost.collected_at <= end_date,
        AnalysisResult.sentiment.isnot(None),
    ]

    # Apply filters to sentiment query
    if filters:
        if filters.board_ids:
            sentiment_query_conditions.append(DBFeedbackPost.board_id.in_(filters.board_ids))
        if filters.tags:
            from sqlalchemy.dialects.postgresql import array
            sentiment_query_conditions.append(DBFeedbackPost.tags.op('&&')(array(filters.tags)))
        if filters.statuses:
            sentiment_query_conditions.append(DBFeedbackPost.status.in_(filters.statuses))
        if filters.sentiment_filter:
            sentiment_query_conditions.append(AnalysisResult.sentiment.in_(filters.sentiment_filter))
        if filters.min_votes is not None:
            sentiment_query_conditions.append(DBFeedbackPost.votes >= filters.min_votes)

    sentiment_query = select(
        AnalysisResult.sentiment,
        func.count(AnalysisResult.id).label("count")
    ).join(
        DBFeedbackPost, AnalysisResult.feedback_post_id == DBFeedbackPost.id
    ).where(*sentiment_query_conditions).group_by(AnalysisResult.sentiment)

    sentiment_results = await session.execute(sentiment_query)
    sentiment_distribution: Dict[str, int] = {
        "Positive": 0,
        "Neutral": 0,
        "Negative": 0,
        "Frustrated": 0,
        "Angry": 0,
    }
    for row in sentiment_results:
        sentiment = row.sentiment
        count = row.count
        if sentiment in sentiment_distribution:
            sentiment_distribution[sentiment] = count

    # Query Jira tickets created
    tickets_created_conditions = [
        DBJiraTicket.created_at >= start_date,
        DBJiraTicket.created_at <= end_date,
    ]

    if filters and filters.jira_project_keys:
        tickets_created_conditions.append(DBJiraTicket.jira_project_key.in_(filters.jira_project_keys))

    tickets_created_query = select(func.count(DBJiraTicket.id)).where(*tickets_created_conditions)
    tickets_created = (await session.execute(tickets_created_query)).scalar() or 0

    # Query Jira tickets resolved
    tickets_resolved_conditions = [
        DBJiraTicket.resolved_at >= start_date,
        DBJiraTicket.resolved_at <= end_date,
        DBJiraTicket.resolved_at.isnot(None),
    ]

    if filters:
        if filters.jira_project_keys:
            tickets_resolved_conditions.append(DBJiraTicket.jira_project_key.in_(filters.jira_project_keys))
        if filters.jira_statuses:
            tickets_resolved_conditions.append(DBJiraTicket.status.in_(filters.jira_statuses))

    tickets_resolved_query = select(func.count(DBJiraTicket.id)).where(*tickets_resolved_conditions)
    tickets_resolved = (await session.execute(tickets_resolved_query)).scalar() or 0

    # Calculate average response time (time from feedback collection to ticket creation)
    response_time_conditions = [
        DBJiraTicket.feedback_post_id == DBFeedbackPost.id,
        DBJiraTicket.created_at >= start_date,
        DBJiraTicket.created_at <= end_date,
    ]

    if filters:
        if filters.board_ids:
            response_time_conditions.append(DBFeedbackPost.board_id.in_(filters.board_ids))
        if filters.jira_project_keys:
            response_time_conditions.append(DBJiraTicket.jira_project_key.in_(filters.jira_project_keys))

    response_time_query = select(
        func.avg(
            func.extract('epoch', DBJiraTicket.created_at - DBFeedbackPost.collected_at) / 3600.0
        )
    ).join(
        DBFeedbackPost, DBJiraTicket.feedback_post_id == DBFeedbackPost.id
    ).where(*response_time_conditions)
    response_time_result = (await session.execute(response_time_query)).scalar()
    average_response_time_hours = float(response_time_result) if response_time_result else None

    # Calculate resolution rate
    total_tickets = tickets_created
    resolution_rate = (tickets_resolved / total_tickets * 100) if total_tickets > 0 else 0.0

    # Calculate average resolution time
    resolution_time_conditions = [
        DBJiraTicket.resolved_at >= start_date,
        DBJiraTicket.resolved_at <= end_date,
        DBJiraTicket.resolved_at.isnot(None),
    ]

    if filters:
        if filters.jira_project_keys:
            resolution_time_conditions.append(DBJiraTicket.jira_project_key.in_(filters.jira_project_keys))
        if filters.jira_statuses:
            resolution_time_conditions.append(DBJiraTicket.status.in_(filters.jira_statuses))

    resolution_time_query = select(
        func.avg(
            func.extract('epoch', DBJiraTicket.resolved_at - DBJiraTicket.created_at) / 3600.0
        )
    ).where(*resolution_time_conditions)
    resolution_time_result = (await session.execute(resolution_time_query)).scalar()
    average_resolution_time_hours = float(resolution_time_result) if resolution_time_result else None

    # Query top priority items (join with analysis results and feedback posts)
    priority_query_conditions = [
        AnalysisResult.feedback_post_id == DBFeedbackPost.id,
        DBFeedbackPost.collected_at >= start_date,
        DBFeedbackPost.collected_at <= end_date,
        AnalysisResult.priority_score.isnot(None),
    ]

    # Apply filters to priority query
    if filters:
        if filters.board_ids:
            priority_query_conditions.append(DBFeedbackPost.board_id.in_(filters.board_ids))
        if filters.tags:
            from sqlalchemy.dialects.postgresql import array
            priority_query_conditions.append(DBFeedbackPost.tags.op('&&')(array(filters.tags)))
        if filters.statuses:
            priority_query_conditions.append(DBFeedbackPost.status.in_(filters.statuses))
        if filters.min_priority_score is not None:
            priority_query_conditions.append(AnalysisResult.priority_score >= filters.min_priority_score)
        if filters.min_votes is not None:
            priority_query_conditions.append(DBFeedbackPost.votes >= filters.min_votes)
        if filters.bug_only:
            priority_query_conditions.append(AnalysisResult.is_bug == True)  # noqa: E712
        if filters.feature_only:
            priority_query_conditions.append(AnalysisResult.is_bug == False)  # noqa: E712
        if filters.sentiment_filter:
            priority_query_conditions.append(AnalysisResult.sentiment.in_(filters.sentiment_filter))

    priority_query = select(
        DBFeedbackPost.title,
        AnalysisResult.priority_score,
        AnalysisResult.analysis_data,
        DBFeedbackPost.canny_post_id,
    ).join(
        AnalysisResult, DBFeedbackPost.id == AnalysisResult.feedback_post_id
    ).where(*priority_query_conditions).order_by(
        AnalysisResult.priority_score.desc()
    ).limit(10)

    priority_results = await session.execute(priority_query)
    priority_items: List[Dict[str, Any]] = []
    for row in priority_results:
        # Extract recommended_jira_priority from analysis_data if available
        recommended_priority = "N/A"
        if row.analysis_data and isinstance(row.analysis_data, dict):
            recommended_priority = row.analysis_data.get("recommended_jira_priority", "N/A")

        priority_items.append({
            "title": row.title,
            "priority_score": row.priority_score,
            "priority": recommended_priority,
            "post_id": row.canny_post_id,
        })

    return DailyMetrics(
        date=report_date,
        new_issues_count=new_issues_count,
        bugs_count=bugs_count,
        feature_requests_count=feature_requests_count,
        bugs_percentage=bugs_percentage,
        sentiment_distribution=sentiment_distribution,
        priority_items=priority_items,
        tickets_created=tickets_created,
        tickets_resolved=tickets_resolved,
        average_response_time_hours=average_response_time_hours,
        resolution_rate=resolution_rate,
        average_resolution_time_hours=average_resolution_time_hours,
    )


def create_report_prompt(metrics: DailyMetrics) -> str:
    """
    Create prompt template for generating natural language report summary.

    Args:
        metrics: DailyMetrics object with aggregated data.

    Returns:
        Formatted prompt string for XAI LLM.
    """
    prompt_parts = [
        "You are a data analyst specializing in customer feedback and product metrics.",
        "Generate a comprehensive, professional daily summary report based on the following metrics.",
        "",
        "## Daily Metrics",
        f"**Date:** {metrics.date.strftime('%Y-%m-%d')}",
        f"**New Issues Reported:** {metrics.new_issues_count}",
        f"**Bugs Identified:** {metrics.bugs_count} ({metrics.bugs_percentage:.1f}%)",
        f"**Feature Requests:** {metrics.feature_requests_count}",
        f"**Jira Tickets Created:** {metrics.tickets_created}",
        f"**Jira Tickets Resolved:** {metrics.tickets_resolved}",
        f"**Resolution Rate:** {metrics.resolution_rate:.1f}%",
    ]

    if metrics.average_response_time_hours:
        prompt_parts.append(f"**Average Response Time:** {metrics.average_response_time_hours:.1f} hours")

    if metrics.average_resolution_time_hours:
        prompt_parts.append(f"**Average Resolution Time:** {metrics.average_resolution_time_hours:.1f} hours")

    if metrics.sentiment_distribution:
        prompt_parts.append("")
        prompt_parts.append("## Sentiment Distribution")
        for sentiment, count in metrics.sentiment_distribution.items():
            if count > 0:
                prompt_parts.append(f"- {sentiment}: {count}")

    if metrics.priority_items:
        prompt_parts.append("")
        prompt_parts.append("## Top Priority Items")
        for item in metrics.priority_items[:10]:  # Top 10
            prompt_parts.append(f"- {item.get('title', 'N/A')} (Priority: {item.get('priority', 'N/A')})")

    prompt_parts.extend(
        [
            "",
            "## Requirements",
            "Generate a professional report summary that includes:",
            "1. **Executive Summary**: A high-level overview (2-3 sentences) of the day's activity",
            "2. **Key Insights**: 3-5 bullet points highlighting important trends, patterns, or concerns",
            "3. **Recommendations**: Actionable recommendations based on the metrics (optional, if insights warrant)",
            "4. **Summary Text**: A complete natural language summary (3-5 paragraphs) covering all key metrics and insights",
            "",
            "Maintain a professional, data-driven tone. Focus on actionable insights and trends.",
            "",
            "## Output Format",
            "**IMPORTANT**: You MUST respond with ONLY a valid JSON object. Do NOT include any Markdown formatting, explanations, or text outside the JSON.",
            "",
            "JSON Schema:",
            "{",
            '  "executive_summary": "string (min 50 chars)",',
            '  "key_insights": ["string", "string", "..."],',
            '  "recommendations": ["string", "string", "..."],',
            '  "summary_text": "string (min 100 chars)"',
            "}",
            "",
            "Generate the JSON report summary now:",
        ]
    )

    return "\n".join(prompt_parts)


def format_report_markdown(metrics: DailyMetrics, summary: ReportSummary) -> str:
    """
    Format report in Markdown with sections.

    Args:
        metrics: DailyMetrics object.
        summary: ReportSummary object with natural language content.

    Returns:
        Formatted Markdown report.
    """
    report_date = metrics.date.strftime("%Y-%m-%d")
    report_parts = [
        f"# Daily Summary Report - {report_date}",
        "",
        "---",
        "",
        "## Executive Summary",
        summary.executive_summary,
        "",
        "---",
        "",
        "## Metrics Overview",
        "",
        "### New Issues",
        f"- **Total New Issues:** {metrics.new_issues_count}",
        f"- **Bugs Identified:** {metrics.bugs_count} ({metrics.bugs_percentage:.1f}%)",
        f"- **Feature Requests:** {metrics.feature_requests_count}",
        "",
        "### Jira Ticket Activity",
        f"- **Tickets Created:** {metrics.tickets_created}",
        f"- **Tickets Resolved:** {metrics.tickets_resolved}",
        f"- **Resolution Rate:** {metrics.resolution_rate:.1f}%",
    ]

    if metrics.average_response_time_hours:
        report_parts.append(f"- **Average Response Time:** {metrics.average_response_time_hours:.1f} hours")

    if metrics.average_resolution_time_hours:
        report_parts.append(f"- **Average Resolution Time:** {metrics.average_resolution_time_hours:.1f} hours")

    if metrics.sentiment_distribution and any(count > 0 for count in metrics.sentiment_distribution.values()):
        report_parts.extend(
            [
                "",
                "### Sentiment Distribution",
            ]
        )
        for sentiment, count in metrics.sentiment_distribution.items():
            if count > 0:
                percentage = (count / metrics.new_issues_count * 100) if metrics.new_issues_count > 0 else 0.0
                report_parts.append(f"- **{sentiment}:** {count} ({percentage:.1f}%)")

    if metrics.priority_items:
        report_parts.extend(
            [
                "",
                "### Top Priority Items",
            ]
        )
        for i, item in enumerate(metrics.priority_items[:10], 1):
            title = item.get("title", "N/A")
            priority = item.get("priority", "N/A")
            report_parts.append(f"{i}. {title} (Priority: {priority})")

    report_parts.extend(
        [
            "",
            "---",
            "",
            "## Key Insights",
        ]
    )
    for insight in summary.key_insights:
        report_parts.append(f"- {insight}")

    if summary.recommendations:
        report_parts.extend(
            [
                "",
                "## Recommendations",
            ]
        )
        for recommendation in summary.recommendations:
            report_parts.append(f"- {recommendation}")

    report_parts.extend(
        [
            "",
            "---",
            "",
            "## Detailed Summary",
            summary.summary_text,
            "",
            "---",
            "",
            f"*Report generated on {datetime.now(UTC).strftime('%Y-%m-%d %H:%M:%S UTC')}*",
        ]
    )

    return "\n".join(report_parts)


class ReportingAgent(BaseAgent):
    """
    Reporting Agent for generating daily summary reports.

    This agent:
    - Queries database for daily metrics
    - Aggregates data (counts, percentages, averages, trends)
    - Uses XAI LLM to generate natural language summaries
    - Formats reports in Markdown
    - Stores reports in database
    """

    def __init__(
        self,
        llm: Optional[ChatXAI] = None,
        deterministic: bool = True,
    ):
        """
        Initialize Reporting Agent.

        Args:
            llm: Optional XAI LLM instance (creates one if not provided).
            deterministic: Whether to enforce deterministic behavior.
        """
        super().__init__(
            name="reporting_agent",
            llm=llm,
            deterministic=deterministic,
        )

    async def generate_daily_report(
        self,
        report_date: Optional[datetime] = None,
        filters: Optional[ReportFilters] = None,
        user_email: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Generate daily summary report with optional filters.

        Args:
            report_date: Date for which to generate report (defaults to yesterday).
            filters: Optional ReportFilters to customize the report.
            user_email: Optional email address of the user who requested the report (for notification).

        Returns:
            Dictionary containing report data including metrics, summary, and formatted content.
        """
        if report_date is None:
            # Default to yesterday
            report_date = (datetime.now(UTC) - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)

        logger.info(
            f"Generating daily report for {report_date.strftime('%Y-%m-%d')}",
            extra={
                "agent_name": self.name,
                "report_date": report_date.isoformat(),
                "has_filters": filters is not None and not filters.is_empty() if filters else False,
            },
        )

        # Query daily metrics with filters
        async with get_session_context() as session:
            metrics = await query_daily_metrics(session, report_date, filters=filters)

        # Generate natural language summary using XAI LLM
        prompt = create_report_prompt(metrics)
        summary = await self.generate_structured_output(
            prompt=prompt,
            schema=ReportSummary,
            system_message="You are a professional data analyst. You MUST respond with valid JSON only. Generate clear, concise, and actionable report summaries in JSON format.",
        )

        # Format report in Markdown
        report_content = format_report_markdown(metrics, summary)

        # Store report in database
        async with get_session_context() as session:
            report = Report(
                report_type="daily",
                report_date=report_date,
                report_content=report_content,
                metrics=metrics.model_dump(mode='json'),  # mode='json' converts datetime to ISO strings
            )
            session.add(report)
            await session.commit()
            await session.refresh(report)

            logger.info(
                f"Stored daily report in database (ID: {report.id})",
                extra={
                    "agent_name": self.name,
                    "report_id": str(report.id),
                    "report_date": report_date.isoformat(),
                },
            )

        # Deliver report via configured channels
        delivery_results = await self._deliver_report(
            report_content=report_content,
            report_date=report_date,
            report_id=str(report.id),
            metrics=metrics.model_dump(),
            user_email=user_email,
        )

        return {
            "report_id": str(report.id),
            "report_date": report_date,
            "metrics": metrics.model_dump(mode='json'),  # mode='json' converts datetime to ISO strings
            "summary": summary.model_dump(),
            "content": report_content,
            "delivery": delivery_results,
        }

    async def _deliver_report(
        self,
        report_content: str,
        report_date: datetime,
        report_id: str,
        metrics: Dict[str, Any],
        user_email: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Deliver report via configured channels (email, file storage, etc.).

        Args:
            report_content: Formatted Markdown report content.
            report_date: Report date.
            report_id: Report ID.
            metrics: Report metrics dictionary.
            user_email: Optional email address of the user who requested the report.

        Returns:
            Dictionary with delivery results for each channel.
        """
        delivery_results: Dict[str, Any] = {
            "email": {"success": False, "error": None},
            "file_storage": {"success": False, "error": None},
            "slack": {"success": False, "error": "Not implemented"},
        }

        try:
            settings = get_settings()
            reporting_settings = settings.reporting

            # Email delivery
            # Determine recipients: user_email takes priority, then configured recipients
            recipients = []
            if user_email:
                recipients.append(user_email)
            if reporting_settings.email_enabled and reporting_settings.recipients:
                recipients.extend(reporting_settings.recipients)
            
            # Remove duplicates
            recipients = list(set(recipients))
            
            logger.info(
                f"Email delivery check - user_email: {user_email}, recipients: {recipients}, email_enabled: {reporting_settings.email_enabled}",
                extra={
                    "agent_name": self.name,
                    "user_email": user_email,
                    "recipients": recipients,
                    "email_enabled": reporting_settings.email_enabled,
                },
            )
            
            if recipients:
                try:
                    email_service = self._get_email_service(settings)
                    if email_service:
                        email_service.send_report_email(
                            to_emails=recipients,
                            report_content=report_content,
                            report_date=report_date.strftime("%Y-%m-%d"),
                        )
                        delivery_results["email"]["success"] = True
                        logger.info(
                            f"Sent report email to {len(recipients)} recipient(s)",
                            extra={
                                "agent_name": self.name,
                                "report_id": report_id,
                                "recipients": recipients,
                                "user_email": user_email,
                            },
                        )
                    else:
                        delivery_results["email"]["error"] = "Email service not configured"
                except Exception as e:
                    delivery_results["email"]["error"] = str(e)
                    logger.error(
                        f"Failed to send report email: {str(e)}",
                        extra={
                            "agent_name": self.name,
                            "report_id": report_id,
                        },
                        exc_info=True,
                    )
            else:
                delivery_results["email"]["error"] = "No recipients configured"

            # File storage
            if reporting_settings.file_storage_enabled:
                try:
                    file_storage = self._get_file_storage_service(settings)
                    if file_storage:
                        file_path = file_storage.save_report(
                            report_content=report_content,
                            report_date=report_date,
                            report_id=report_id,
                            metadata={"metrics": metrics},
                        )
                        delivery_results["file_storage"]["success"] = True
                        delivery_results["file_storage"]["file_path"] = file_path
                        logger.info(
                            f"Saved report to file: {file_path}",
                            extra={
                                "agent_name": self.name,
                                "report_id": report_id,
                                "file_path": file_path,
                            },
                        )
                    else:
                        delivery_results["file_storage"]["error"] = "File storage not configured"
                except Exception as e:
                    delivery_results["file_storage"]["error"] = str(e)
                    logger.error(
                        f"Failed to save report to file: {str(e)}",
                        extra={
                            "agent_name": self.name,
                            "report_id": report_id,
                        },
                        exc_info=True,
                    )

            # Slack delivery (placeholder for future implementation)
            if reporting_settings.slack_enabled:
                delivery_results["slack"]["error"] = "Slack integration not yet implemented"

        except Exception as e:
            logger.error(
                f"Error in report delivery: {str(e)}",
                extra={
                    "agent_name": self.name,
                    "report_id": report_id,
                },
                exc_info=True,
            )

        return delivery_results

    def _get_email_service(self, settings) -> Optional[EmailService]:
        """
        Get or create EmailService instance from settings.

        Args:
            settings: Application settings.

        Returns:
            EmailService instance if configured, None otherwise.
        """
        email_settings = settings.email

        if not email_settings.smtp_host:
            logger.debug("Email service not configured (no SMTP host)")
            return None

        return EmailService(
            smtp_host=email_settings.smtp_host,
            smtp_port=email_settings.smtp_port,
            smtp_username=email_settings.smtp_username,
            smtp_password=email_settings.smtp_password.get_secret_value() if email_settings.smtp_password else None,
            use_tls=email_settings.use_tls,
            from_email=email_settings.from_email,
        )

    def _get_file_storage_service(self, settings) -> Optional[FileStorageService]:
        """
        Get or create FileStorageService instance from settings.

        Args:
            settings: Application settings.

        Returns:
            FileStorageService instance.
        """
        file_storage_settings = settings.file_storage

        return FileStorageService(
            base_path=file_storage_settings.base_path,
            create_dirs=file_storage_settings.create_dirs,
        )

    async def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the reporting agent (required by BaseAgent).
        
        This is a placeholder implementation since ReportingAgent doesn't use
        the standard state-based workflow pattern. It operates independently
        to generate reports.

        Args:
            state: Not used for ReportingAgent (kept for compatibility).

        Returns:
            Dummy state (ReportingAgent doesn't modify workflow state).
        """
        # ReportingAgent doesn't use the standard state workflow
        # It's invoked directly via generate_daily_report()
        logger.warning(
            "ReportingAgent.execute() called but this agent doesn't use state-based workflow. "
            "Use generate_daily_report() directly instead.",
            extra={"agent_name": self.name},
        )
        return state

    async def run(self, report_date: Optional[datetime] = None) -> Dict[str, Any]:
        """
        Run the reporting agent (wrapper for generate_daily_report).

        Args:
            report_date: Date for which to generate report.

        Returns:
            Dictionary containing report data.
        """
        logger.info(
            "Starting reporting agent execution",
            extra={"agent_name": self.name},
        )

        try:
            result = await self.generate_daily_report(report_date)
            logger.info(
                "Reporting agent execution completed",
                extra={"agent_name": self.name},
            )
            return result
        except Exception as e:
            error_msg = f"Reporting agent execution failed: {str(e)}"
            logger.error(
                error_msg,
                extra={"agent_name": self.name},
                exc_info=True,
            )
            raise


def get_reporting_agent() -> ReportingAgent:
    """
    Get or create Reporting Agent instance.

    Returns:
        ReportingAgent instance.
    """
    return ReportingAgent()

